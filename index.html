<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Arnav Jain, Nilesh Gupta, Aditya Kusupati, Jon Barron, Deepak Pathak and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f5b461;
  text-decoration:none;
  }
  body,td,th {
     font-family: 'Source Sans Pro', Arial, sans-serif;
     font-size: 16px;
     font-weight: 400
  }
  heading {
     font-family: 'Source Sans Pro', Arial, sans-serif;
     font-size: 19px;
     font-weight: 600
  }
  strong {
     font-family: 'Source Sans Pro', Arial, sans-serif;
     font-size: 16px;
     font-weight: 800
  }
  strongred {
     font-family: 'Source Sans Pro', Arial, sans-serif;
     color: 'red' ;
     font-size: 16px
  }
  sectionheading {
     font-family: 'Source Sans Pro', Arial, sans-serif;
     font-size: 22px;
     font-weight: 600
  }
  pageheading {
     font-family: 'Source Sans Pro', Arial, sans-serif;
     font-size: 60px;
     font-weight: 400
  }
  </style>
  <!-- <link rel="icon" type="image/png" href="images/W.png"> -->
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Arnav Kumar Jain</title>
  <link rel = "icon" href = "images/profile_pic.png" type = "image/x-icon">
  <meta name="Arnav Jain's Homepage" http-equiv="Content-Type" content="Arnav Jain's Homepage">
  <!-- link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'> -->
  <link href="https://fonts.googleapis.com/css2?family=Source+Sans+Pro:wght@200;300;400;600;700&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=PT+Sans:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <script type="text/javascript" src="js/toggle.js"></script>
  <!-- Start : Google Analytics Code -->
  <!-- <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90394621-1', 'auto');
  ga('send', 'pageview');

  </script> -->
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-RQW73WXYW9"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-RQW73WXYW9');
  </script>
  <script type="text/javascript" async 
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script> 
</head>

<body>
<table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
<tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
	<p align="left">
		<center><pageheading>Arnav Kumar Jain</pageheading><br>
		<b>&nbsp;Email</b>: <a href="mailto:arnavkj95@gmail.com">arnavkj95@gmail.com</a></center>
	</p>

	<tr>
		<td width="32%" valign="top"><a href="#Bio">
			<img src="images/me.jpg" width="100%" style="border-radius:15px"></a>
			<p align=center>
				<a href="resources/CV.pdf" target="_blank">CV</a> | <a href="https://scholar.google.com/citations?hl=en&user=tu7wKckAAAAJ" target="_blank">Scholar</a> | <a href="https://github.com/arnavkj1995" target="_blank">Github</a> | <a href="https://twitter.com/arnavkj95" target="_blank">Twitter</a>
			</p>
		</td>

		<td width="68%" valign="top" align="justify" id="Bio">
			I am a Ph.D. student at <a href="https://www.umontreal.ca/" target="_blank">Université de Montréal</a> and <a href="https://mila.quebec/" target="_blank">Mila</a> advised by <a href="https://mila.quebec/en/person/irina-rish/" target="_blank">Prof. Irina Rish</a>, and closely collaborating with <a href="https://sanjibanc.github.io/" target="_blank">Sanjiban Choudhury</a>. 
      I am interested in developing efficient decision-making agents, with my work focusing on imitation from few demonstrations and exploration with limited interactions.
      <br><br>
      Prior to joining PhD, I was a Data & Applied Scientist at Microsoft working closely with <a href="http://manikvarma.org/" target="_blank">Dr. Manik Varma</a> at Microsoft Research India on web-scale algorithms for recommender system.
      Before that, I earned my Integrated M.Sc. in Mathematics and Computing from <a href="http://www.iitkgp.ac.in/" target="_blank">Indian Institute of Technology Kharagpur</a>. I also spent time in <a href="http://krssg.in/">KRSSG</a> working on path planning algorithms for autonomous soccer playing robots. 
    </td>
	</tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
	<tr><td id="Publications"> <sectionheading>&nbsp;&nbsp;Publications</sectionheading><div style="float: right;"></div></td></tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
		<td width="33%" valign="top" align="center"><a href="pubs/Jain25.pdf" target="_blank"><img src="images/Jain25.gif" alt="EPL" width="100%" style="border-radius:1px"></a></td>
		<td width="67%" valign="top">
			<p>
        <a href="pubs/Jain25.pdf" target="_blank" id="EPL">
				<heading>Multi-Turn Code Generation Through Single-Step Rewards </heading></a><br>
				<strong>Arnav Kumar Jain</strong><sup>*</sup>, 
          <a href="https://gonzalogonzalezpumariega.com/">Gonzalo Gonzalez-Pumariega<sup>*</sup></a>,
          <a href="https://www.linkedin.com/in/waynechen314/">Wayne Chen</a>,</span>
          <a href="https://rush-nlp.com/">Alexander M Rush</a>,
          <a href="https://wenting-zhao.github.io/">Wenting Zhao</a>,
          <a href="https://sanjibanc.github.io/" target="_blank">Sanjiban Choudhury</a><br>
			</p>	
      Preprint.<br>
			<!-- </p> -->
			<div class="paper" id="Jain25">
				<a href="javascript:toggleblock('Jain25abs')">abstract</a> /
				<a shape="rect" href="javascript:togglebib('Jain25')" class="togglebib">bibtex</a> /
				<a href="pubs/Jain25.pdf" target="_blank">pdf</a> /
        <a href="https://github.com/portal-cornell/muCode" target="_blank">code</a> /
				<a href="https://portal-cornell.github.io/muCode/" target="_blank">website</a>
				<br>
				<p align="justify" > <i id="Jain25abs">  We address the problem of code generation from multi-turn execution feedback. 
          Existing methods either generate code without feedback or use complex, hierarchical reinforcement learning to optimize multi-turn rewards.
          We propose a simple yet scalable approach, \( \mu \texttt{Code} \), that solves multi-turn code generation using only single-step rewards.
          Our key insight is that code generation is a one-step recoverable MDP, where the correct code can be recovered from any intermediate code state in a single turn.
          \( \mu \texttt{Code} \) iteratively trains both a generator to provide  code solutions conditioned on multi-turn execution feedback and a verifier to score the newly generated code.
          Experimental evaluations show that our approach achieves significant improvements over the state-of-the-art generator-verifier baselines. 
          We provide analysis of the design choices of the reward models and policy, and show the efficacy of \( \mu \texttt{Code} \) at utilizing the execution feedback. </i></p>
        
        <pre xml:space="preserve">
          @article{jain2025multi,
            title={Multi-Turn Code Generation Through Single Step Rewards},
            author={Arnav Kumar Jain and Gonzalo Gonzalez-Pumariega and Wayne Chen and Alexander M Rush and Wenting Zhao and Sanjiban Choudhury},
            journal={CoRR},
            volume={abs/2502.20380},
            year={2025}
          } 
        </pre>
      </div>
    </td>
  </tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
		<td width="33%" valign="top" align="center"><a href="pubs/Jain24.pdf" target="_blank"><img src="images/Jain24.png" alt="EPL" width="100%" style="border-radius:1px"></a></td>
		<td width="67%" valign="top">
			<p>
        <a href="pubs/Jain24.pdf" target="_blank" id="EPL">
				<heading>Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching </heading></a><br>
				<strong>Arnav Kumar Jain</strong>, <a href="https://harwiltz.github.io/" target="_blank">Harley Wiltzer</a>, <a href="https://brosa.ca/" target="_blank">Jesse Farebrother</a>, <a href="https://irina-rish.com/" target="_blank">Irina Rish</a>, <a href="https://neo-x.github.io/" target="_blank">Glen Berseth</a>, <a href="https://sanjibanc.github.io/" target="_blank">Sanjiban Choudhury</a><br>
			</p>	
      International Conference on Learning Representations (<b>ICLR</b>), 2025 <br>
      Models of Human Feedback for AI Alignment Workshop @ ICML, 2024<br>
			<!-- </p> -->
			<div class="paper" id="Jain24">
				<a href="javascript:toggleblock('Jain24abs')">abstract</a> /
				<a shape="rect" href="javascript:togglebib('Jain24')" class="togglebib">bibtex</a> /
				<a href="pubs/Jain24.pdf" target="_blank">pdf</a> /
				<a href="https://github.com/arnavkj1995/SFM" target="_blank">code</a>
        <a href="https://arnavkj1995.github.io/SFM/" target="_blank">website</a>
				<br>
				<p align="justify" > <i id="Jain24abs"> In inverse reinforcement learning (IRL), an agent seeks to replicate expert demonstrations through interactions with the environment. Traditionally, IRL is treated as an adversarial game, where an adversary searches over reward models, and a learner optimizes the reward through repeated RL procedures. This game-solving approach is both computationally expensive and difficult to stabilize. Instead, we embrace a more fundamental perspective of IRL as that of state-occupancy matching: by matching the cumulative state features encountered by the expert, the agent can match the returns of the expert under any reward function in a hypothesis class. We present a simple yet novel framework for IRL where a policy greedily matches successor features of the expert where successor features efficiently compute the expected features of successive states observed by the agent. Our non-adversarial method does not require learning a reward function and can be solved seamlessly with existing value-based reinforcement learning algorithms. Remarkably, our approach works in state-only settings without expert action labels, a setting which behavior cloning (BC) cannot solve. Empirical results demonstrate that our method learns from as few as a single expert demonstration and achieves comparable performance on various control tasks. </i></p>
        
        <pre xml:space="preserve">
          @inproceedings{
              jain2025nonadversarial,
              title={Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching},
              author={Arnav Kumar Jain and Harley Wiltzer and Jesse Farebrother and Irina Rish and Glen Berseth and Sanjiban Choudhury},
              booktitle={The Thirteenth International Conference on Learning Representations},
              year={2025},
              url={https://openreview.net/forum?id=LvRQgsvd5V}
            }
        </pre>
      </div>
    </td>
  </tr>
  
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
		<td width="33%" valign="top" align="center"><a href="pubs/Jain23.pdf" target="_blank"><img src="images/Jain23.png" alt="EPL" width="100%" style="border-radius:1px"></a></td>
		<td width="67%" valign="top">
			<p>
        <a href="pubs/Jain23.pdf" target="_blank" id="EPL">
				<heading>Maximum State Entropy Exploration using Predecessor and Successor Representations </heading></a><br>
				<strong>Arnav Kumar Jain</strong>, <a href="https://lucaslehnert.github.io/" target="_blank">Lucas Lehnert</a>, <a href="https://irina-rish.com/" target="_blank">Irina Rish</a>, <a href="https://neo-x.github.io/" target="_blank">Glen Berseth</a><br>
			</p>	
      Neural Information Processing Systems (<b>NeurIPS</b>), 2023 <br>
        Frontiers4LCD Workshop @ ICML, 2023<br>
			<!-- </p> -->
			<div class="paper" id="Jain23">
				<a href="javascript:toggleblock('Jain23abs')">abstract</a> /
				<a shape="rect" href="javascript:togglebib('Jain23')" class="togglebib">bibtex</a> /
				<a href="pubs/Jain23.pdf" target="_blank">pdf</a> /
				<a href="https://github.com/arnavkj1995/Eta_Psi_Learning" target="_blank">code</a>
				<br>
				<p align="justify" > <i id="Jain23abs"> Animals have a developed ability to explore that aids them in important tasks such as locating food, exploring for shelter, and finding misplaced items. These exploration skills necessarily track where they have been so that they can plan for finding items with relative efficiency. Contemporary exploration algorithms often learn a less efficient exploration strategy because they either condition only on the current state or simply rely on making random open-loop exploratory moves. In this work, we propose ηψ-Learning, a method to learn efficient exploratory policies by conditioning on past episodic experience to make the next exploratory move. Specifically, ηψ-Learning learns an exploration policy that maximizes the entropy of the state visitation distribution of a single trajectory. Furthermore, we demonstrate how variants of the predecessor representation and successor representations can be combined to predict the state visitation entropy. Our experiments demonstrate the efficacy of ηψ-Learning to strategically explore the environment and maximize the state coverage with limited samples. </i></p>
        
        <pre xml:space="preserve">
          @inproceedings{
              jain2023maximum,
              title={Maximum State Entropy Exploration using Predecessor and Successor Representations},
              author={Arnav Kumar Jain and Lucas Lehnert and Irina Rish and Glen Berseth},
              booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
              year={2023},
              url={https://openreview.net/forum?id=tFsxtqGmkn}
            }
        </pre>
      </div>
    </td>
  </tr>
  
  <tr>
		<td width="33%" valign="top" align="center"><a href="pubs/Jain22.pdf" target="_blank"><img src="images/Jain22.png" alt="VSG" width="100%" style="border-radius:1px"></a></td>
		<td width="67%" valign="top">
			<p>
        <a href="pubs/Jain22.pdf" target="_blank" id="VSG">
				<heading>Learning Robust Dynamics through Variational Sparse Gating</heading></a><br>
				<strong>Arnav Kumar Jain</strong>, <a href="https://shivakanthsujit.github.io/" target="_blank">Shivakanth Sujit</a>, <a href="https://shrutij01.github.io/" target="_blank">Shruti Joshi</a>, <a href="http://www-ens.iro.umontreal.ca/~michals/" target="_blank">Vincent Michalski</a>, <a href="https://danijar.com/">Danijar Hafner</a> and <a href="https://saebrahimi.github.io/" target="_blank">Samira-Ebrahimi Kahou</a><br>
			</p>	
        Neural Information Processing Systems (<b>NeurIPS</b>), 2022 <br>
        DeepRL Workshop @ NeurIPS, 2021<br>
			<!-- </p> -->
			<div class="paper" id="Jain22">
				<a href="javascript:toggleblock('Jain22abs')">abstract</a> /
				<a shape="rect" href="javascript:togglebib('Jain22')" class="togglebib">bibtex</a> /
				<a href="pubs/Jain22.pdf" target="_blank">pdf</a> /
				<a href="https://github.com/arnavkj1995/VSG" target="_blank">code</a>
				<br>
				<p align="justify" > <i id="Jain22abs"> Learning world models from their sensory inputs enables agents to plan for actions by imagining their future outcomes. World models have previously been shown to improve sample-efficiency in simulated environments with few objects, but have not yet been applied successfully to environments with many objects. In environments with many objects, often only a small number of them are moving or interacting at the same time. In this paper, we investigate integrating this inductive bias of sparse interactions into the latent dynamics of world models trained from pixels. First, we introduce Variational Sparse Gating (VSG), a latent dynamics model that updates its feature dimensions sparsely through stochastic binary gates. Moreover, we propose a simplified architecture Simple Variational Sparse Gating (SVSG) that removes the deterministic pathway of previous models, resulting in a fully stochastic transition function that leverages the VSG mechanism. We evaluate the two model architectures in the BringBackShapes (BBS) environment that features a large number of moving objects and partial observability, demonstrating clear improvements over prior models. </i></p>
        
        <pre xml:space="preserve">
          @InProceedings{Jain22,
            author    = "Jain, A.~K. and Sujit, S. and 
                        Joshi, S. and Michalski, V. 
                        and Hafner, D. and Kahou, S.~E.",
            title     = "Learning Robust Dynamics through 
                          Variational Sparse Gating",
            booktitle = {Advances in 
              Neural Information Processing Systems},
            month     = {December},
            year      = {2022}
          }
        </pre>
      </div>
    </td>
  </tr>

  <tr>
		<td width="33%" valign="top" align="center"><a href="pubs/Saini21.pdf" target="_blank"><img src="images/Saini21.jpg" alt="GalaXC" width="100%" style="border-radius:1px"></a></td>
		<td width="67%" valign="top">
			<p>
        <a href="pubs/Saini21.pdf" target="_blank" id="GalaXC">
				<heading>GalaXC: Graph Neural Networks with Labelwise Attention for Extreme Classification</heading></a><br>
				<a href="http://deepaksaini119.github.io/" target="_blank">Deepak Saini</a>*, <strong>Arnav Kumar Jain</strong>*, Kushal Dave*, Jian Jiao, Amit Singh, Ruofei Zhang and <a href="http://manikvarma.org" target="_blank">Manik Varma</a><br>
			</p>
      	
      The Web Conference (<b>TheWebConf</b>), 2021<br>			
			<div class="paper" id="Saini21">
				<a href="javascript:toggleblock('Saini21abs')">abstract</a> /
				<a shape="rect" href="javascript:togglebib('Saini21')" class="togglebib">bibtex</a> /
				<a href="pubs/Saini21.pdf" target="_blank">pdf</a> /
				<a href="https://github.com/Extreme-classification/GalaXC" target="_blank">code</a>
				<br>
				<p align="justify" > <i id="Saini21abs">This paper develops the GalaXC algorithm for Extreme Classification, where the task is to annotate a document with the most relevant subset of labels from an extremely large label set. Extreme classification has been successfully applied to several real world web-scale applications such as web search, product recommendation, query rewriting, etc. GalaXC identifies two critical deficiencies in leading extreme classification algorithms. First, existing approaches generally assume that documents and labels reside in disjoint sets, even though in several applications, labels and documents cohabit the same space. Second, several approaches, albeit scalable, do not utilize various forms of metadata offered by applications, such as label text and label correlations. To remedy these, GalaXC presents a framework that enables collaborative learning over joint document-label graphs at massive scales, in a way that naturally allows various auxiliary sources of information, including label metadata, to be incorporated. GalaXC also introduces a novel label-wise attention mechanism to meld high-capacity extreme classifiers with its framework. An efficient end-to-end implementation of GalaXC is presented that could be trained on a dataset with 50M labels and 97M training documents in less than 100 hours on 4xV100 GPUs. This allowed GalaXC to not only scale to applications with several millions of labels, but also be up to 18% more accurate than leading deep extreme classifiers, while being upto 2-50x faster to train and 10x faster to predict on benchmark datasets. GalaXC is particularly well-suited to warm-start scenarios where predictions need to be made on data points with partially revealed label sets, and was found to be up to 25% more accurate than extreme classification algorithms specifically designed for warm start settings. In A/B tests conducted on the Bing search engine, GalaXC could improve the Click Yield (CY) and coverage by 1.52% and 1.11% respectively. Code for GalaXC is available at <a href="https://github.com/Extreme-classification/GalaXC">https://github.com/Extreme-classification/GalaXC</a>.</i></p>

        <pre xml:space="preserve">
              @InProceedings{Saini21,
                author    = "Saini, D. and Jain, A.~K. and Dave, K. 
                            and Jiao, J. and Singh, A. and Zhang, R.
                            and Varma, M.",
                title     = "GalaXC: Graph neural networks with 
                  labelwise attention for extreme classification",
                booktitle = "Proceedings of The ACM International 
                  World Wide Web Conference",
                month     = "April",
                year      = "2021",
              }
          </pre>
	      </div>
     	</td>
  	</tr>

    <tr>
    <td width="33%" valign="top" align="center"><a href="pubs/Lahiri20.pdf" target="_blank"><img src="images/Lahiri20.gif" alt="PriorGAN" width="100%" style="border-radius:1px"></a></td>
    <td width="67%" valign="top">
      <p>
        <a href="pubs/Lahiri20.pdf" target="_blank" id="PriorGAN">
        <heading>Prior Guided GAN Based Semantic Inpainting</heading></a><br>
        <a href="https://sites.google.com/site/aviseklahiri/home" target="_blank">Avisek Lahiri*</a>, <strong>Arnav Kumar Jain</strong>*, Sanskar Agrawal, <a href="https://cse.iitkgp.ac.in/~pabitra/" target="_blank">Pabitra Mitra</a>, and <a href="http://www.iitkgp.ac.in/department/EC/faculty/ec-pkb" target="_blank">Prabir Kumar Biswas</a> <br>
      </p>
      
      Computer Vision and Patten Recognition (<b>CVPR</b>), 2020<br>
      <!-- Long Oral presentation<br> -->
      <div class="paper" id="lahiri2020prior">
        <a href="javascript:toggleblock('Lahiri20abs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('lahiri2020prior')" class="togglebib">bibtex</a> /
        <a href="pubs/Lahiri20.pdf" target="_blank">pdf</a> /
        <a href="resources/Lahiri20slides.pdf" target="_blank">slides</a>
        <br>
        <p align="justify" > <i id="Lahiri20abs">Contemporary deep learning based semantic inpainting can be approached from two directions. First, and the more explored,  approach is to train an offline deep regression network over the masked pixels with an additional refinement by adversarial training. This approach requires a single feed-forward pass for inpainting at inference. Another promising, yet unexplored approach is to first train a generative model to map a latent prior distribution to natural image manifold and during inference time search for the `best-matching' prior to reconstruct the signal. The primary aversion towards the latter genre is due to its inference time iterative optimization and difficulty to scale to higher resolution. In this paper, going against the general trend, we focus on the second paradigm of inpainting and address both of its mentioned problems. Most importantly, we learn a data driven parametric network to directly predict a matching prior for a given masked image. This converts an iterative paradigm to a single feed forward inference pipeline with around 800x speedup.  We also regularize our network with structural prior (computed from the masked image itself) which helps in better preservation of pose and size of the object to be inpainted. Moreover, to extend our model for sequence reconstruction, we propose a recurrent net based grouped latent prior learning. Finally, we leverage recent advancements in high resolution GAN training to scale our inpainting network to 256x256. Experiments (spanning across resolutions from 64x64 to 256x256) conducted on SVHN, Standford Cars, CelebA, CelebA-HQ and ImageNet image datasets, and FaceForensics video datasets reveal that we consistently improve upon contemporary benchmarks from both schools of approaches.</a>.</i></p>
        <pre xml:space="preserve">
@inproceedings{lahiri2020prior,
  title     = {Prior Guided GAN Based Semantic Inpainting},
  author    = {Lahiri, Avisek and Jain, Arnav Kumar and Agrawal, 
    Sanskar and Mitra, Pabitra and Biswas, Prabir Kumar},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer 
    Vision and Pattern Recognition},
  pages     = {13696--13705},
  year      = {2020}
}
          </pre>
        </div>
      </td>
    </tr>
    <tr>
    <td width="33%" valign="top" align="center"><a href="pubs/Lahiri19.pdf" target="_blank"><img src="images/Lahiri19.png" alt="SemanticGAN" width="100%" style="border-radius:1px"></a></td>
    <td width="67%" valign="top">
      <p>
        <a href="pubs/Lahiri19.pdf" target="_blank" id="SemanicGAN">
        <heading>Faster unsupervised semantic inpainting: A GAN based approach</heading></a><br>
        <a href="https://sites.google.com/site/aviseklahiri/home" target="_blank">Avisek Lahiri*</a>, <strong>Arnav Kumar Jain</strong>*, Divyasri Nadendla, and <a href="http://www.iitkgp.ac.in/department/EC/faculty/ec-pkb" target="_blank">Prabir Kumar Biswas</a> <br>
      </p>  
      IEEE International Conference on Image Processing (<b>ICIP</b>), 2019 <br>
      <!-- </p> -->
      <!-- Long Oral presentation<br> -->
      <div class="paper" id="lahiri2019prior">
        <a href="javascript:toggleblock('Lahiri19abs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('lahiri2019prior')" class="togglebib">bibtex</a> /
        <a href="pubs/Lahiri19.pdf" target="_blank">pdf</a>
        <br>
        <p align="justify" > <i id="Lahiri19abs">In this paper, we propose to improve the inference speed and visual quality of contemporary baseline of Generative Adversarial Networks (GAN) based unsupervised semantic inpainting. This is made possible with better initialization of the core iterative optimization involved in the framework. To our best knowledge, this is also the first attempt of GAN based video inpainting with consideration to temporal cues. On single image inpainting, we achieve about 4.5-5x speedup and 80x on videos compared to baseline. Simultaneously, our method has better spatial and temporal reconstruction qualities as found on three image and one video dataset.</a>.</i></p>
        <pre xml:space="preserve">
@inproceedings{lahiri2019faster,
  title        = {Faster Unsupervised Semantic Inpainting: 
    A GAN Based Approach},
  author       ={Lahiri, Avisek and Jain, Arnav Kumar and Nadendla, 
    Divyasri and Biswas, Prabir Kumar},
  booktitle    = {2019 IEEE International Conference on Image 
    Processing (ICIP)},
  pages        = {2706--2710},
  year         = {2019},
  organization = {IEEE}
}
          </pre>
        </div>
      </td>
    </tr>

        <tr>
    <td width="33%" valign="top" align="center"><a href="pubs/Agarwalla18.pdf" target="_blank"><img src="images/Agarwalla18.png" alt="KRSSG" width="100%" style="border-radius:1px"></a></td>
    <td width="67%" valign="top">
      <p>
        <a href="pubs/Agarwalla18.pdf" target="_blank" id="KRSSG">
        <heading>Bayesian Optimisation with Prior Reuse for Motion Planning in Robot Soccer</heading></a><br>
        <a href="https://abhinavagarwalla.github.io/" target="_blank">Abhinav Agarwalla*</a>, <strong>Arnav Kumar Jain</strong>*, <a href="https://kvmanohar22.github.io/" target="_blank">KV Manohar</a>, Arpit Tarang Saxena, and <a href="http://www.facweb.iitkgp.ac.in/~jay/" target="_blank">Jayanta Mukhopadhyay</a> <br>
      </p> 
      
      Conference on Data Science and Management of Data (<b>CoDS-COMAD</b>), 2018 <br>
      <!-- </p> -->
      <!-- Long Oral presentation<br> -->
      <div class="paper" id="agarwalla2018bayesian">
        <a href="javascript:toggleblock('Agarwalla18abs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('agarwalla2018bayesian')" class="togglebib">bibtex</a> /
        <a href="pubs/Agarwalla18.pdf" target="_blank">pdf</a>
        <br>
        <p align="justify" > <i id="Agarwalla18abs">We integrate learning and motion planning for soccer playing differential drive robots using Bayesian optimisation. Trajectories generated using end-slope cubic Bézier splines are first optimised globally through Bayesian optimisation for a set of candidate points with obstacles. The optimised trajectories along with robot and obstacle positions and velocities are stored in a database. The closest planning situation is identified from the database using k-Nearest Neighbour approach. It is further optimised online through reuse of prior information from previously optimised trajectory. Our approach reduces computation time of trajectory optimisation considerably. Velocity profiling generates velocities consistent with robot kinodynamoic constraints, and avoids collision and slipping. Extensive testing is done on developed simulator as well as on physical differential drive robots. Our method shows marked improvements in mitigating tracking error, and reducing traversal and computational time over competing techniques under the constraints of performing tasks in real time.</a>.</i></p>
        <pre xml:space="preserve">
@inproceedings{agarwalla2018bayesian,
  title     = {Bayesian optimisation with prior reuse 
    for motion planning in robot soccer},
  author    = {Agarwalla, Abhinav and Jain, Arnav Kumar 
    and Manohar, KV and Saxena, Arpit Tarang and 
    Mukhopadhyay, Jayanta},
  booktitle = {Proceedings of the ACM India Joint 
    International Conference on Data Science and 
    Management of Data},
  pages     = {88--97},
  year      = {2018}
}

          </pre>
        </div>
      </td>
    </tr>

        <tr>
    <td width="33%" valign="top" align="center"><a href="pubs/Jain17.pdf" target="_blank"><img src="images/Jain17.jpg" alt="RecurrentMemory" width="100%" style="border-radius:1px"></a></td>
    <td width="67%" valign="top">
      <p>
        <a href="pubs/Jain17.pdf" target="_blank" id="RecurrentMemory">
        <heading>Recurrent Memory Addressing for describing videos</heading></a><br>
        <strong>Arnav Kumar Jain</strong>*, <a href="https://abhinavagarwalla.github.io/" target="_blank">Abhinav Agarwalla*</a>, <a href="https://people.eecs.berkeley.edu/~krishna/" target="_blank">Kumar Krishna Agrawal*</a>, and <a href="https://cse.iitkgp.ac.in/~pabitra/" target="_blank">Pabitra Mitra</a> <br>
      </p> 
      
      Deep Vision Workshop at Computer Vision and Pattern Recognition (<b>CVPRW</b>), 2017 <br>
      <!-- </p> -->
      <!-- Long Oral presentation<br> -->
      <div class="paper" id="jain2017recurrent">
        <a href="javascript:toggleblock('Jain17abs')">abstract</a> /
        <a shape="rect" href="javascript:togglebib('jain2017recurrent')" class="togglebib">bibtex</a> /
        <a href="pubs/Jain17.pdf" target="_blank">pdf</a>
        <br>
        <p align="justify" > <i id="Jain17abs">Deep Neural Network architectures with external memory components allow the model to perform inference and capture long term dependencies by storing information explicitly. In this paper, we generalize Key-Value Memory Networks to a multimodal setting and introduce a novel key-addressing mechanism to deal with sequence-to-sequence models. <br>The advantages of the framework are demonstrated on the task of generating natural language descriptions for videos. The proposed model naturally decomposes the problem of video captioning into vision and language segments, dealing with them as key-value pairs. More specifically, we learn a semantic embedding (v) corresponding to each keyframe (k) in the video, thereby creating (k, v) memory slots. We propose to find the attention weights, conditioned on the previous attention distributions for the key-value memory slots in the memory addressing schema. Exploiting this flexibility of the framework, we capture spatial dependencies while mapping from the visual to semantic embedding. Experiments done on the Youtube2Text dataset demonstrate usefulness of this recurrent key-addressing, while achieving competitive scores on BLEU@4, METEOR metrics against state-of-the-art models.</a>.</i></p>
        <pre xml:space="preserve">
@inproceedings{jain2017recurrent,
  title     = {Recurrent Memory Addressing for Describing Videos.},
  author    = {Jain, Arnav Kumar and Agarwalla, Abhinav and 
    Agrawal, Kumar Krishna and Mitra, Pabitra},
  booktitle = {CVPR Workshops},
  volume    = {7},
  year      = {2017}
}
          </pre>
        </div>
      </td>
    </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
     <tr><td><br><p align="right"><font size="2">
     Template: <a href="https://nilesh2797.github.io">this</a>,
     <a href="https://jonbarron.info">this</a>, <a href="https://people.eecs.berkeley.edu/~pathak/">this</a> and <a href="https://homes.cs.washington.edu/~kusupati//">this</a>
     </font></p></td></tr>
</table>

</td></tr>
</table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('Jain25abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('Jain24abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('Jain23abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('Jain22abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('Saini21abs');
</script>
<script xml:space="preserve" language="JavaScript">
    hideblock('Lahiri20abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('Lahiri19abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('Agarwalla18abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('Jain17abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('acknowledgements');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('gradschool');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('quotes');
</script>

</body>

</html>
